from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext, VectorStoreIndex, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
import chromadb
import torch
from transformers import BitsAndBytesConfig

print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"Current GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

CHROMADB_PATH = r'C:\Users\liangxianguang\Desktop\nwpu\bigmodel\test\chroma'
COLLECTION_NAME = 'public_security_law'
MODEL_PATH = r'D:\llama index\sentence-transformers\paraphrase-multilingual-MiniLM-L12-v2'
LLM_PATH = r'C:\Users\liangxianguang\Desktop\model\deepseek-ai\DeepSeek-R1-Distill-Qwen-1___5B'

# ä¿®å¤ï¼šä½¿ç”¨æ–°çš„é‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

# ä¿®å¤ï¼šç§»é™¤å†²çªçš„ max_new_tokens å‚æ•°
llm = HuggingFaceLLM(
    model_name=LLM_PATH,
    tokenizer_name=LLM_PATH,
    context_window=4096,  # ä½¿ç”¨ LlamaIndex çš„æ ‡å‡†å‚æ•°
    max_new_tokens=512,   # ä½¿ç”¨ LlamaIndex çš„æ ‡å‡†å‚æ•°ï¼Œè€Œä¸æ˜¯ generate_kwargs
    model_kwargs={
        'trust_remote_code': True,
        'torch_dtype': torch.float16,
        'quantization_config': quantization_config,
        'low_cpu_mem_usage': True,
    },
    tokenizer_kwargs={'trust_remote_code': True},
    device_map='auto',
    # ç®€åŒ–ç”Ÿæˆå‚æ•°ï¼Œé¿å…å†²çª
    generate_kwargs={
        'temperature': 0.1,
        'do_sample': True,
        'top_p': 0.9,
        'top_k': 50,
        'repetition_penalty': 1.1,
        # ç§»é™¤ max_new_tokens å’Œ pad_token_id
    }
)

# åˆå§‹åŒ– embedding æ¨¡å‹ (ä½¿ç”¨ GPU)
embeddings = HuggingFaceEmbedding(
    model_name=MODEL_PATH,
    device='cuda'
)

# è®¾ç½®å…¨å±€æ¨¡å‹
Settings.llm = llm
Settings.embed_model = embeddings

# åˆ›å»ºæœ¬åœ°æŒä¹…åŒ– ChromaDB å®¢æˆ·ç«¯
chroma_client = chromadb.PersistentClient(path=CHROMADB_PATH)

# æ£€æŸ¥ç°æœ‰é›†åˆ
existing_collections = chroma_client.list_collections()
print("ç°æœ‰é›†åˆ:", [c.name for c in existing_collections])

# å®‰å…¨è·å–é›†åˆ
try:
    chroma_collection = chroma_client.get_collection(name=COLLECTION_NAME)
    print(f"âœ… æˆåŠŸè·å–é›†åˆ: {COLLECTION_NAME}")
except chromadb.errors.NotFoundError:
    print(f"âŒ é›†åˆ {COLLECTION_NAME} ä¸å­˜åœ¨")
    exit(1)

# åˆ›å»º ChromaVectorStore
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

# åˆ›å»ºç´¢å¼•å’ŒæŸ¥è¯¢å¼•æ“
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)

# åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    similarity_top_k=3,
    response_mode="compact",
)

# æ‰§è¡ŒæŸ¥è¯¢
query = "ç…½åŠ¨ã€ç­–åˆ’éæ³•é›†ä¼šã€æ¸¸è¡Œã€ç¤ºå¨ï¼Œä¸å¬åŠé˜»çš„å¤„ç†ï¼Ÿ"
print(f"\nğŸ” æŸ¥è¯¢: {query}")
print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")

try:
    response = query_engine.query(query)
    print(f"\nâœ… å›ç­”:\n{response}")
    
    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æºæ–‡æ¡£ï¼ˆè°ƒè¯•ç”¨ï¼‰- æ·»åŠ åˆ†æ•°æ˜¾ç¤º
    print(f"\nğŸ“š ç›¸å…³æ³•æ¡:")
    if hasattr(response, 'source_nodes'):
        for i, node in enumerate(response.source_nodes):
            # å°è¯•è·å–ç›¸ä¼¼åº¦åˆ†æ•°
            score = getattr(node, 'score', None)
            score_text = f" (åˆ†æ•°: {score:.4f})" if score is not None else ""
            
            print(f"  {i+1}. {node.metadata.get('full_title', 'æœªçŸ¥')}{score_text}")
            print(f"     å†…å®¹: {node.text[:100]}...")
    else:
        print("  æœªæ‰¾åˆ°æºæ–‡æ¡£ä¿¡æ¯")
            
except Exception as e:
    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()